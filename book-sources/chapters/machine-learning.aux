\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Machine Learning}{237}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Unsupervised Learning}{237}{section.16.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Dimensionality Reduction and PCA}{237}{subsection.16.1.1}}
\newlabel{eq-emp-cov}{{16.1}{237}{Dimensionality Reduction and PCA}{equation.16.1.1}{}}
\newlabel{eq-cov-approx}{{16.2}{237}{Dimensionality Reduction and PCA}{equation.16.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces  Empirical covariance of the data and its associated singular values. }}{238}{figure.16.1}}
\newlabel{fig-cov}{{16.1}{238}{Empirical covariance of the data and its associated singular values}{figure.16.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces PCA main axes capture variance}}{238}{figure.16.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces  2-D and 3-D PCA vizualization of the input clouds. }}{239}{figure.16.3}}
\newlabel{fig-pca}{{16.3}{239}{2-D and 3-D PCA vizualization of the input clouds}{figure.16.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}Clustering and $k$-means}{239}{subsection.16.1.2}}
\@writefile{toc}{\contentsline {paragraph}{$k$-means}{239}{section*.135}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.4}{\ignorespaces $k$-means clusters according to Vornoi cells.}}{239}{figure.16.4}}
\newlabel{eq-kmeans-1}{{16.3}{239}{$k$-means}{equation.16.1.3}{}}
\newlabel{eq-kmeans-2}{{16.4}{239}{$k$-means}{equation.16.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.5}{\ignorespaces  Left: iteration of $k$-means algorithm. Right: histogram of points belonging to each class after the $k$-means optimization. }}{240}{figure.16.5}}
\newlabel{fig-kmeans}{{16.5}{240}{Left: iteration of $k$-means algorithm. Right: histogram of points belonging to each class after the $k$-means optimization}{figure.16.5}{}}
\@writefile{toc}{\contentsline {paragraph}{$k$-means++}{240}{section*.136}}
\@writefile{toc}{\contentsline {paragraph}{Lloyd algorithm and continuous densities.}{240}{section*.137}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.6}{\ignorespaces  Iteration of $k$-means algorithm (Lloyd algorithm) on continuous densities $\mu $. Top: uniform. Bottom: non-uniform (the densities of $\mu $ with respect to the Lebesgue measure is displayed as a grayscale image in the background). }}{241}{figure.16.6}}
\newlabel{fig-lloyd}{{16.6}{241}{Iteration of $k$-means algorithm (Lloyd algorithm) on continuous densities $\mu $. Top: uniform. Bottom: non-uniform (the densities of $\mu $ with respect to the Lebesgue measure is displayed as a grayscale image in the background)}{figure.16.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Empirical Risk Minimization}{241}{section.16.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}Empirical Risk}{241}{subsection.16.2.1}}
\newlabel{eq-erm-1}{{16.5}{241}{Empirical Risk}{equation.16.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}Prediction and Consistency}{241}{subsection.16.2.2}}
\newlabel{eq-consistency-estim}{{16.6}{241}{Prediction and Consistency}{equation.16.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.3}Parametric Approaches and Regularization}{242}{subsection.16.2.3}}
\newlabel{eq-erm-param}{{16.7}{242}{Parametric Approaches and Regularization}{equation.16.2.7}{}}
\newlabel{eq-consistency-param}{{16.8}{242}{Parametric Approaches and Regularization}{equation.16.2.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Prediction vs. estimation risks.}{242}{section*.138}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.4}Testing Set and Cross-validation}{242}{subsection.16.2.4}}
\newlabel{eq-valid-risk}{{16.9}{242}{Testing Set and Cross-validation}{equation.16.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.8}{\ignorespaces  Conditional expectation. }}{243}{figure.16.8}}
\newlabel{fig-bound-regul}{{16.8}{243}{Conditional expectation}{figure.16.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Supervised Learning: Regression}{243}{section.16.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.7}{\ignorespaces Probabilistic modelling.}}{243}{figure.16.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}Linear Regression}{243}{subsection.16.3.1}}
\newlabel{sec-linear-models}{{16.3.1}{243}{Linear Regression}{subsection.16.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Least square and conditional expectation.}{243}{section*.139}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.9}{\ignorespaces Linear regression.}}{244}{figure.16.9}}
\@writefile{toc}{\contentsline {paragraph}{Penalized linear models.}{244}{figure.16.9}}
\newlabel{eq-erm-lin}{{16.10}{244}{Penalized linear models}{equation.16.3.10}{}}
\newlabel{eq-empirical-conver}{{16.11}{244}{Penalized linear models}{equation.16.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Ridge regression (quadratic penalization).}{245}{section*.141}}
\newlabel{eq-linest-std}{{16.12}{245}{Ridge regression (quadratic penalization)}{equation.16.3.12}{}}
\newlabel{eq-linest-woodbury}{{16.13}{245}{Ridge regression (quadratic penalization)}{equation.16.3.13}{}}
\newlabel{eq-sc-stat}{{16.14}{245}{}{equation.16.3.14}{}}
\newlabel{eq-rate-estim}{{16.15}{245}{}{equation.16.3.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Supervised Learning: Classification}{245}{section.16.4}}
\newlabel{sec-classif}{{16.4}{245}{Supervised Learning: Classification}{section.16.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.1}Nearest Neighbors Classification}{245}{subsection.16.4.1}}
\newlabel{sec-nn-classif}{{16.4.1}{245}{Nearest Neighbors Classification}{subsection.16.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.10}{\ignorespaces Nearest neighbors.}}{245}{figure.16.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.11}{\ignorespaces  $k$-nearest-neighbor classification boundary function. }}{246}{figure.16.11}}
\newlabel{fig-hist-classif}{{16.11}{246}{$k$-nearest-neighbor classification boundary function}{figure.16.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.2}Two Classes Logistic Classification}{246}{subsection.16.4.2}}
\newlabel{sec-two-class-logit}{{16.4.2}{246}{Two Classes Logistic Classification}{subsection.16.4.2}{}}
\newlabel{eq-two-class-logit-model}{{16.16}{246}{Two Classes Logistic Classification}{equation.16.4.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.12}{\ignorespaces  1-D and 2-D logistic classification, showing the impact of $|\tmspace  -\thinmuskip {.1667em}| \beta  |\tmspace  -\thinmuskip {.1667em}|$ on the sharpness of the classification boundary. }}{247}{figure.16.12}}
\newlabel{fig-losses}{{16.12}{247}{1-D and 2-D logistic classification, showing the impact of $\norm {\be }$ on the sharpness of the classification boundary}{figure.16.12}{}}
\newlabel{eq-logistic-optim}{{16.17}{247}{Two Classes Logistic Classification}{equation.16.4.17}{}}
\newlabel{eq-logistic-loss}{{16.18}{247}{Two Classes Logistic Classification}{equation.16.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.13}{\ignorespaces  Comparison of loss functions. }}{248}{figure.16.13}}
\newlabel{fig-losses}{{16.13}{248}{Comparison of loss functions}{figure.16.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.14}{\ignorespaces  Influence on the separation distance between the class on the classification probability. }}{248}{figure.16.14}}
\newlabel{fig-separation-influ}{{16.14}{248}{Influence on the separation distance between the class on the classification probability}{figure.16.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.3}Multi-Classes Logistic Classification}{248}{subsection.16.4.3}}
\newlabel{sec-multiclass-logit}{{16.4.3}{248}{Multi-Classes Logistic Classification}{subsection.16.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.15}{\ignorespaces  2-D and 3-D PCA vizualization of the digits images. }}{249}{figure.16.15}}
\newlabel{fig-digits}{{16.15}{249}{2-D and 3-D PCA vizualization of the digits images}{figure.16.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.16}{\ignorespaces  Results of digit classification Left: probability $h(x)_\ell $ of belonging to each of the 9 first classes (displayed over a 2-D PCA space). Right: colors reflect probability $h(x)$ of belonging to classes. }}{250}{figure.16.16}}
\newlabel{fig-digits-classes}{{16.16}{250}{Results of digit classification Left: probability $h(x)_\ell $ of belonging to each of the 9 first classes (displayed over a 2-D PCA space). Right: colors reflect probability $h(x)$ of belonging to classes}{figure.16.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}Kernel Methods}{250}{section.16.5}}
\newlabel{sec-kernel-methods}{{16.5}{250}{Kernel Methods}{section.16.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.1}Reproducing Kernel Hilbert Space}{250}{subsection.16.5.1}}
\newlabel{eq-kernel-generic}{{16.19}{251}{}{equation.16.5.19}{}}
\newlabel{eq-rkhs-representer}{{16.20}{251}{}{equation.16.5.20}{}}
\newlabel{eq-rkhs-variational}{{16.21}{251}{}{equation.16.5.21}{}}
\newlabel{eq-kernel-interp}{{16.22}{251}{Reproducing Kernel Hilbert Space}{equation.16.5.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.17}{\ignorespaces  Regression using a Gaussian kernel. }}{252}{figure.16.17}}
\newlabel{fig-kernel}{{16.17}{252}{Regression using a Gaussian kernel}{figure.16.17}{}}
\newlabel{eq-gauss-kernel}{{16.23}{252}{Reproducing Kernel Hilbert Space}{equation.16.5.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.2}Examples of Kernelized Algorithms}{252}{subsection.16.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Kernelized ridge regression.}{252}{section*.142}}
\@writefile{toc}{\contentsline {paragraph}{Kernelized logistic classification.}{252}{section*.143}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.18}{\ignorespaces  Non-linear classification using a Gaussian kernel. }}{253}{figure.16.18}}
\newlabel{fig-classes-kernel}{{16.18}{253}{Non-linear classification using a Gaussian kernel}{figure.16.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Kernelized nearest-neihbors. }{253}{section*.144}}
\@writefile{toc}{\contentsline {paragraph}{Kernel on strings. }{253}{section*.145}}
\@setckpt{chapters/machine-learning}{
\setcounter{page}{254}
\setcounter{equation}{23}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{16}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{18}
\setcounter{table}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{3}
\setcounter{bookmark@seq@number}{251}
\setcounter{parentequation}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{thm}{51}
\setcounter{prop}{53}
\setcounter{defn}{22}
\setcounter{cor}{0}
\setcounter{alg}{0}
\setcounter{lem}{6}
\setcounter{rem}{8}
\setcounter{exmp}{9}
\setcounter{float@type}{32}
\setcounter{listing}{0}
\setcounter{lstnumber}{1}
\setcounter{mdf@globalstyle@cnt}{0}
\setcounter{mdfcountframes}{0}
\setcounter{mdf@env@i}{0}
\setcounter{mdf@env@ii}{0}
\setcounter{mdf@zref@counter}{0}
\setcounter{section@level}{4}
\setcounter{lstlisting}{0}
}
